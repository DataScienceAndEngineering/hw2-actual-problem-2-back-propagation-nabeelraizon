{"cells":[{"cell_type":"markdown","metadata":{"id":"xuuA_SHMuUne"},"source":["# HW2: Problem 2: Working out Backpropagation\n","\n","Read Chapter 2 of Michael Nielsen's article/book from top to bottom:\n","\n","* [http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)\n","\n","He outlines a few exersizes in that article which you must complete. Do the following a, b, c:\n","\n","a. He invites you to write out a proof of equation BP3\n","\n","b. He invites you to write out a proof of equation BP4\n","\n","c. He proposes that you code a fully matrix-based approach to backpropagation over a mini-batch. Implement this with explanation where you change the notation so that instead of having a bias term, you assume that the input variables are augmented with a \"column\" of \"1\"s, and that the weights $w_0$.\n","\n","Your submission should be a single jupyter notebook. Use markdown cells with latex for equations of a jupyter notebook for each proof for \"a.\" and \"b.\". Make sure you include text that explains your steps. Next for \"c\" this is an implementation problem. You need to understand and modify the code the Michael Nielsen provided so that instead it is using a matrixed based approach. Again don't keep the biases separate. After reading data in (use the iris data set), create a new column corresponding to $x_0=1$, and as mentioned above and discussed in class (see notes) is that the bias term can then be considered a weight $w_0$. Again use markdown cells around your code and comments to explain your work. Test the code on the iris data set with 4 node input (5 with a constant 1), three hidden nodes, and three output nodes, one for each species/class."]},{"cell_type":"markdown","metadata":{"id":"NsfIQN1-uwGB"},"source":["## a. Proof of Michael Nielsons equation BP3\n"]},{"cell_type":"markdown","metadata":{},"source":["  Proof of Equation BP3: By definition, $\\delta_{lj} = \\frac{\\partial C}{\\partial z_{lj}}$\n","    <br> Also, the weighted input is defined as $z_{lj} = \\sum_k w_{ljk}a_{l-1k} + b_{lj}$ <br> Therefore, $\\frac{\\partial C}{\\partial b_{lj}} = \\frac{\\partial C}{\\partial z_{lj}} \\frac{\\partial z_{lj}}{\\partial b_{lj}} = \\delta_{lj} \\cdot 1 = \\delta_{lj}$"]},{"cell_type":"markdown","metadata":{"id":"axsx4eu9u_3x"},"source":["## b. Proof of Michael Nielsons equation BP4\n"]},{"cell_type":"markdown","metadata":{},"source":[" Proof of Equation BP4: Again, $\\delta_{lj} = \\frac{\\partial C}{\\partial z_{lj}}$ by definition.<br>\n"," Also, $z_{l+1j} = \\sum_k w_{(l+1)jk}a_{lj} + b_{(l+1)j}$\n","\n","Taking the derivative: $\\frac{\\partial z_{l+1j}}{\\partial w_{ljk}} = a_{lj}$\n","\n","Then applying the chain rule: $\\frac{\\partial C}{\\partial w_{ljk}} = \\frac{\\partial C}{\\partial z_{l+1j}} \\frac{\\partial z_{l+1j}}{\\partial w_{ljk}} = \\delta_{l+1j} \\cdot a_{lj}$\n","\n","This proves Equation BP4.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rj1HbN9RvVXe"},"source":["## c. Using both markdown cells and code cells implement that you code a fully matrix-based approach to backpropagation over a mini-batch. Implement this with explanation where you change the notation so that instead of having a bias term, you assume that the input variables are augmented with a \"column\" of \"1\"s, and that the weights $w_0$."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1676928890900,"user":{"displayName":"Michael Grossberg","userId":"10616474120098361836"},"user_tz":300},"id":"3wUek4Nau7x7"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sigmoid_prime(x):\n","    return sigmoid(x) * (1 - sigmoid(x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = np.array([[0.1, 0.2],\n","              [0.3, 0.4],\n","              [0.5, 0.6]])\n","\n","Y = np.array([[0.1, 0.2],\n","             [0.3, 0.5],\n","             [0.5, 0.7]])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["W1 = np.array([[0.1, 0.2],\n","               [0.3, 0.4]])\n","W2 = np.array([[0.1, 0.2],\n","               [0.3, 0.4]])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["b1 = np.array([0.3, 0.4])\n","b2 = np.array([0.0, 0.0])"]},{"cell_type":"markdown","metadata":{},"source":["X contains the input data with 2 samples and 2 features\n","W1 and b1 are weights and biases for layer 1.\n","Z1 calculates the pre-activation for layer 1 by: Input X multiplied by weights W1, added to biases b1\n","A1 calculates activation for layer 1 by applying sigmoid function to Z1\n","Similarly, W2 and b2 are weights and biases for layer 2\n","Z2 calculates pre-activation for layer 2 using previous layer's activation A1\n","A2 is the final output, calculated by applying sigmoid on Z2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Z1 = X @ W1 + b1\n","A1 = sigmoid(Z1)\n","\n","Z2 = A1 @ W2 + b2\n","A2 = sigmoid(Z2)"]},{"cell_type":"markdown","metadata":{},"source":["The goal here is to update the weights and biases to reduce the loss between predictions A2 and true targets Y.\n","\n","The loss derivative w.r.t A2 is calculated first.\n","\n","Then chain rule is applied to propagate derivatives backwards:\n","\n","Derivative of loss w.r.t W2 is calculated by multiplying derivative w.r.t A2 with A1\n","Similarily for b2 and Z2\n","Z1's derivative depends on Z2's derivative via chain rule\n","Then W1 and b1 derivatives are calculated"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dC_dA2 = (A2-Y) * sigmoid_prime(Z2)\n","dC_dW2 = A1.T @ dC_dA2\n","dC_db2 = np.sum(dC_dA2, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dC_dZ1 = dC_dA2 @ W2.T * sigmoid_prime(Z1)\n","dC_dW1 = X.T @ dC_dZ1\n","dC_db1 = np.sum(dC_dZ1, axis=0)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPGRo7E+vvs1+9PrlkZvkBC","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
